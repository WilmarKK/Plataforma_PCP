"""
Sistema de Valida√ß√£o Inteligente Avan√ßado para Dados de Produ√ß√£o Industrial
===========================================================================

Este sistema usa IA para detectar padr√µes automaticamente e se adaptar a diferentes:
- Tipos de m√°quinas (conhecidas e desconhecidas)
- Processos variados
- Formatos de dados diferentes
- Padr√µes de produ√ß√£o √∫nicos

Funciona como uma verdadeira "IA interna" que aprende com os dados.
"""

import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, Counter
import json
from pathlib import Path

class SeverityLevel(Enum):
    INFO = "INFO"
    WARNING = "WARNING" 
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

@dataclass
class ValidationResult:
    severity: SeverityLevel
    message: str
    row_index: int
    machine: str
    operator: str
    timestamp: str
    suggestions: List[str]
    confidence: float = 1.0  # Nova: confian√ßa da IA na detec√ß√£o
    pattern_type: str = "generic"  # Nova: tipo de padr√£o detectado

@dataclass
class MachineProfile:
    """Perfil inteligente de uma m√°quina, aprendido automaticamente"""
    name: str
    setup_times: Dict[str, List[int]] = field(default_factory=dict)  # padr√£o -> tempos observados
    production_speeds: List[float] = field(default_factory=list)  # velocidades observadas
    common_processes: Counter = field(default_factory=Counter)  # processos mais comuns
    typical_operators: set = field(default_factory=set)  # operadores habituais
    shift_patterns: Dict[int, List[str]] = field(default_factory=dict)  # hora -> operadores
    setup_keywords: List[str] = field(default_factory=list)  # palavras que indicam setup
    production_keywords: List[str] = field(default_factory=list)  # palavras que indicam produ√ß√£o
    anomaly_thresholds: Dict[str, float] = field(default_factory=dict)  # limites adaptativos
    last_updated: datetime = field(default_factory=datetime.now)

class SmartProductionValidator:
    """
    Validador inteligente que aprende padr√µes automaticamente
    """
    
    def __init__(self, learning_enabled: bool = True):
        self.results = []
        self.learning_enabled = learning_enabled
        self.machine_profiles: Dict[str, MachineProfile] = {}
        self.global_patterns = {
            'time_formats': [],
            'setup_indicators': Counter(),
            'production_indicators': Counter(),
            'common_anomalies': Counter()
        }
        
        # Padr√µes base que a IA vai expandir
        self.base_setup_patterns = [
            r'acerto', r'setup', r'entrada', r'prepara√ß√£o', r'prep',
            r'faca\s*nova', r'nova\s*faca', r'troca', r'ajuste',
            r'hot\s*stamping', r'verniz', r'destaque', r'relevo'
        ]
        
        self.base_time_patterns = [
            r'(\d{1,2})\s*h\s*(\d{1,2})?\s*min?',  # 1h 30min
            r'(\d{1,2}):(\d{2})',                   # 1:30
            r'(\d{1,3})\s*min',                     # 90min
            r'(\d{1,2})\s*h',                       # 2h
            r'(\d{1,4})\s*p/?h'                     # 5000p/h
        ]
    
    def analyze_and_learn(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Primeira fase: analisa os dados para aprender padr√µes
        """
        df = self._normalize_columns(df)
        learning_report = {
            'machines_discovered': set(),
            'patterns_learned': {},
            'anomalies_detected': 0,
            'confidence_level': 0.0
        }
        
        # Descobrir m√°quinas automaticamente
        machines = self._discover_machines(df)
        learning_report['machines_discovered'] = machines
        
        # Aprender padr√µes de cada m√°quina
        for machine in machines:
            machine_data = df[df['maquina'].str.contains(machine, case=False, na=False)]
            profile = self._learn_machine_patterns(machine, machine_data)
            self.machine_profiles[machine] = profile
            learning_report['patterns_learned'][machine] = {
                'setup_keywords': profile.setup_keywords[:5],  # top 5
                'avg_production_speed': np.mean(profile.production_speeds) if profile.production_speeds else 0,
                'common_processes': dict(profile.common_processes.most_common(3))
            }
        
        # Calcular confian√ßa geral
        total_rows = len(df)
        learning_report['confidence_level'] = min(1.0, total_rows / 100)  # mais dados = mais confian√ßa
        
        return learning_report
    
    def validate_with_ai(self, df: pd.DataFrame) -> List[ValidationResult]:
        """
        Segunda fase: valida usando os padr√µes aprendidos
        """
        self.results = []
        df = self._normalize_columns(df)
        
        # Se ainda n√£o aprendeu, aprende primeiro
        if not self.machine_profiles:
            self.analyze_and_learn(df)
        
        # Valida√ß√µes inteligentes por linha
        for idx, row in df.iterrows():
            self._ai_validate_row(idx, row, df)
        
        # Valida√ß√µes de sequ√™ncia inteligentes
        self._ai_validate_sequences(df)
        
        # Detectar anomalias usando ML simples
        self._detect_statistical_anomalies(df)
        
        return sorted(self.results, key=lambda x: (x.severity.value, x.confidence))
    
    def _discover_machines(self, df: pd.DataFrame) -> set:
        """Descobre tipos de m√°quina automaticamente"""
        machines = set()
        
        # Procurar em colunas relevantes
        for col in ['maquina', 'equipamento', 'recurso']:
            if col in df.columns:
                values = df[col].dropna().str.lower()
                for value in values:
                    # Extrair palavras-chave de m√°quinas
                    words = re.findall(r'\b\w+\b', str(value))
                    for word in words:
                        if len(word) > 3 and word not in ['equipamento', 'maquina']:
                            machines.add(word)
        
        # Filtrar m√°quinas reais (aparecem pelo menos 3 vezes)
        machine_counts = Counter()
        for machine in machines:
            count = df['maquina'].str.contains(machine, case=False, na=False).sum()
            if count >= 3:
                machine_counts[machine] = count
        
        return set(machine_counts.keys())
    
    def _learn_machine_patterns(self, machine_name: str, machine_data: pd.DataFrame) -> MachineProfile:
        """Aprende padr√µes espec√≠ficos de uma m√°quina"""
        profile = MachineProfile(name=machine_name)
        
        for idx, row in machine_data.iterrows():
            # Aprender padr√µes de setup
            evento = str(row.get('evento', '')).lower()
            processo = str(row.get('processo', '')).lower()
            tempo = str(row.get('tempo', ''))
            
            # Identificar opera√ß√µes de setup
            combined_text = f"{evento} {processo}"
            if self._is_likely_setup(combined_text):
                setup_time = self._extract_time_minutes(tempo)
                if setup_time > 0:
                    setup_key = self._extract_setup_key(combined_text)
                    if setup_key not in profile.setup_times:
                        profile.setup_times[setup_key] = []
                    profile.setup_times[setup_key].append(setup_time)
                
                # Aprender palavras-chave de setup
                for word in combined_text.split():
                    if len(word) > 3 and word not in profile.setup_keywords:
                        profile.setup_keywords.append(word)
            
            # Aprender velocidades de produ√ß√£o
            if self._is_likely_production(combined_text):
                qtd_produzida = self._safe_float(row.get('qtd_produzida', 0))
                tempo_minutos = self._extract_time_minutes(tempo)
                if qtd_produzida > 0 and tempo_minutos > 0:
                    speed = (qtd_produzida / tempo_minutos) * 60  # pe√ßas/hora
                    if 100 < speed < 50000:  # filtrar valores absurdos
                        profile.production_speeds.append(speed)
            
            # Aprender padr√µes de operadores
            operator = str(row.get('operador', '')).strip()
            if operator:
                profile.typical_operators.add(operator)
                
                # Padr√µes de turno
                try:
                    start_time = pd.to_datetime(row.get('inicio'))
                    hour = start_time.hour
                    if hour not in profile.shift_patterns:
                        profile.shift_patterns[hour] = []
                    if operator not in profile.shift_patterns[hour]:
                        profile.shift_patterns[hour].append(operator)
                except:
                    pass
            
            # Contar processos comuns
            if processo:
                profile.common_processes[processo] += 1
        
        # Calcular limites adaptativos
        if profile.production_speeds:
            speeds = np.array(profile.production_speeds)
            profile.anomaly_thresholds['speed_low'] = np.percentile(speeds, 10)
            profile.anomaly_thresholds['speed_high'] = np.percentile(speeds, 90)
            profile.anomaly_thresholds['speed_mean'] = np.mean(speeds)
        
        # Limites de setup baseados em dados hist√≥ricos
        all_setup_times = []
        for times in profile.setup_times.values():
            all_setup_times.extend(times)
        
        if all_setup_times:
            setup_array = np.array(all_setup_times)
            profile.anomaly_thresholds['setup_low'] = np.percentile(setup_array, 5)
            profile.anomaly_thresholds['setup_high'] = np.percentile(setup_array, 95)
            profile.anomaly_thresholds['setup_mean'] = np.mean(setup_array)
        
        return profile
    
    def _is_likely_setup(self, text: str) -> bool:
        """Determina se o texto indica opera√ß√£o de setup"""
        setup_score = 0
        
        # Padr√µes base
        for pattern in self.base_setup_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                setup_score += 2
        
        # Outros indicadores
        if re.search(r'\b(prep|config|ajust|troca|mud)\w*', text, re.IGNORECASE):
            setup_score += 1
        
        if re.search(r'\b(nova|novo|new)\b', text, re.IGNORECASE):
            setup_score += 1
        
        return setup_score >= 2
    
    def _is_likely_production(self, text: str) -> bool:
        """Determina se o texto indica opera√ß√£o de produ√ß√£o"""
        return any(word in text for word in ['produ√ß√£o', 'producao', 'fabrica√ß√£o', 'fabricacao', 'opera√ß√£o', 'operacao'])
    
    def _extract_setup_key(self, text: str) -> str:
        """Extrai chave representativa do tipo de setup"""
        # Palavras importantes para categorizar setup
        key_words = []
        for word in text.split():
            if len(word) > 3 and word not in ['acerto', 'setup', 'prepara√ß√£o']:
                key_words.append(word)
        
        return ' '.join(key_words[:2]) if key_words else 'generic'
    
    def _ai_validate_row(self, idx: int, row: pd.Series, df: pd.DataFrame):
        """Valida√ß√£o inteligente de uma linha"""
        machine = self._identify_machine(row)
        
        # Valida√ß√µes base
        self._check_time_consistency(idx, row)
        self._check_data_completeness(idx, row)
        
        # Valida√ß√µes espec√≠ficas da m√°quina (se conhecida)
        if machine in self.machine_profiles:
            profile = self.machine_profiles[machine]
            self._ai_check_setup_anomalies(idx, row, profile)
            self._ai_check_production_anomalies(idx, row, profile)
            self._ai_check_operator_patterns(idx, row, profile)
        else:
            # M√°quina desconhecida - usar padr√µes gen√©ricos
            self._generic_anomaly_detection(idx, row)
    
    def _identify_machine(self, row: pd.Series) -> str:
        """Identifica inteligentemente o tipo de m√°quina"""
        machine_field = str(row.get('maquina', '')).lower()
        
        # Buscar correspond√™ncia com m√°quinas conhecidas
        for known_machine in self.machine_profiles.keys():
            if known_machine in machine_field:
                return known_machine
        
        # Extrair nome mais prov√°vel da m√°quina
        words = re.findall(r'\b\w+\b', machine_field)
        for word in words:
            if len(word) > 3:
                return word
        
        return 'unknown'
    
    def _ai_check_setup_anomalies(self, idx: int, row: pd.Series, profile: MachineProfile):
        """Detec√ß√£o inteligente de anomalias de setup"""
        evento = str(row.get('evento', '')).lower()
        processo = str(row.get('processo', '')).lower()
        tempo_str = str(row.get('tempo', '00:00'))
        combined = f"{evento} {processo}"
        
        if self._is_likely_setup(combined):
            tempo_minutos = self._extract_time_minutes(tempo_str)
            setup_key = self._extract_setup_key(combined)
            
            # Comparar com hist√≥rico da m√°quina
            if setup_key in profile.setup_times:
                historical_times = profile.setup_times[setup_key]
                mean_time = np.mean(historical_times)
                std_time = np.std(historical_times)
                
                confidence = min(1.0, len(historical_times) / 10)  # mais dados = mais confian√ßa
                
                if tempo_minutos < mean_time - 2 * std_time:
                    self._add_ai_result(
                        SeverityLevel.INFO,
                        f"Setup muito r√°pido ({tempo_minutos}min vs m√©dia {mean_time:.0f}min)",
                        idx, row, confidence, "statistical_anomaly",
                        [f"Tempo usual para '{setup_key}': {mean_time:.0f}¬±{std_time:.0f}min",
                         "Verificar se foi reaproveitamento ou erro de apontamento"]
                    )
                elif tempo_minutos > mean_time + 2 * std_time:
                    self._add_ai_result(
                        SeverityLevel.WARNING,
                        f"Setup muito lento ({tempo_minutos}min vs m√©dia {mean_time:.0f}min)",
                        idx, row, confidence, "statistical_anomaly",
                        [f"Tempo usual para '{setup_key}': {mean_time:.0f}¬±{std_time:.0f}min",
                         "Investigar problemas t√©cnicos ou complexidade especial"]
                    )
            
            # Usar limites gerais da m√°quina
            elif 'setup_mean' in profile.anomaly_thresholds:
                mean_setup = profile.anomaly_thresholds['setup_mean']
                if tempo_minutos > mean_setup * 2:
                    self._add_ai_result(
                        SeverityLevel.WARNING,
                        f"Setup fora do padr√£o da m√°quina ({tempo_minutos}min vs m√©dia {mean_setup:.0f}min)",
                        idx, row, 0.7, "machine_pattern",
                        ["Baseado no hist√≥rico geral da m√°quina",
                         "Verificar se processo √© realmente mais complexo"]
                    )
    
    def _ai_check_production_anomalies(self, idx: int, row: pd.Series, profile: MachineProfile):
        """Detec√ß√£o inteligente de anomalias de produ√ß√£o"""
        evento = str(row.get('evento', '')).lower()
        
        if self._is_likely_production(f"{evento} {str(row.get('processo', ''))}"):
            tempo_minutos = self._extract_time_minutes(str(row.get('tempo', '00:00')))
            qtd_produzida = self._safe_float(row.get('qtd_produzida', 0))
            
            if tempo_minutos > 0 and qtd_produzida > 0:
                prod_real = (qtd_produzida / tempo_minutos) * 60  # pe√ßas/hora
                
                if 'speed_mean' in profile.anomaly_thresholds:
                    speed_mean = profile.anomaly_thresholds['speed_mean']
                    speed_low = profile.anomaly_thresholds.get('speed_low', speed_mean * 0.5)
                    speed_high = profile.anomaly_thresholds.get('speed_high', speed_mean * 1.5)
                    
                    confidence = min(1.0, len(profile.production_speeds) / 20)
                    
                    if prod_real < speed_low:
                        self._add_ai_result(
                            SeverityLevel.WARNING,
                            f"Produtividade baixa: {prod_real:.0f} p/h (m√©dia: {speed_mean:.0f})",
                            idx, row, confidence, "performance_anomaly",
                            [f"Faixa normal: {speed_low:.0f}-{speed_high:.0f} p/h",
                             "Investigar problemas de qualidade ou paradas"]
                        )
                    elif prod_real > speed_high:
                        self._add_ai_result(
                            SeverityLevel.INFO,
                            f"Produtividade alta: {prod_real:.0f} p/h (m√©dia: {speed_mean:.0f})",
                            idx, row, confidence, "performance_anomaly",
                            [f"Faixa normal: {speed_low:.0f}-{speed_high:.0f} p/h",
                             "Verificar se n√£o houve erro nos dados"]
                        )
    
    def _ai_check_operator_patterns(self, idx: int, row: pd.Series, profile: MachineProfile):
        """Verifica padr√µes inteligentes de operadores"""
        operador = str(row.get('operador', '')).strip()
        if not operador:
            return
        
        try:
            inicio = pd.to_datetime(row.get('inicio'))
            hour = inicio.hour
            
            # Verificar se operador √© conhecido para esta m√°quina
            if operador not in profile.typical_operators:
                confidence = min(1.0, len(profile.typical_operators) / 5)
                self._add_ai_result(
                    SeverityLevel.INFO,
                    f"Operador incomum para esta m√°quina: {operador}",
                    idx, row, confidence, "operator_pattern",
                    [f"Operadores habituais: {', '.join(list(profile.typical_operators)[:3])}",
                     "Verificar se houve treinamento ou substitui√ß√£o"]
                )
            
            # Verificar padr√µes de turno
            if hour in profile.shift_patterns:
                usual_operators = profile.shift_patterns[hour]
                if operador not in usual_operators:
                    confidence = min(1.0, len(usual_operators) / 3)
                    self._add_ai_result(
                        SeverityLevel.INFO,
                        f"Operador fora do turno habitual ({hour:02d}h): {operador}",
                        idx, row, confidence, "shift_pattern",
                        [f"Operadores usuais neste hor√°rio: {', '.join(usual_operators)}",
                         "Pode indicar hora extra ou cobertura"]
                    )
        
        except Exception:
            pass
    
    def _generic_anomaly_detection(self, idx: int, row: pd.Series):
        """Detec√ß√£o gen√©rica para m√°quinas desconhecidas"""
        tempo_str = str(row.get('tempo', '00:00'))
        tempo_minutos = self._extract_time_minutes(tempo_str)
        
        # Limites muito b√°sicos
        if tempo_minutos > 480:  # mais de 8 horas
            self._add_ai_result(
                SeverityLevel.WARNING,
                f"Opera√ß√£o muito longa: {tempo_minutos/60:.1f}h",
                idx, row, 0.5, "generic_time",
                ["Verificar se opera√ß√£o realmente durou esse tempo",
                 "Considerar paradas n√£o registradas"]
            )
        
        # Verificar produ√ß√£o zero com tempo
        qtd_produzida = self._safe_float(row.get('qtd_produzida', 0))
        if tempo_minutos > 30 and qtd_produzida == 0:
            evento = str(row.get('evento', '')).lower()
            if 'produ√ß√£o' in evento or 'producao' in evento:
                self._add_ai_result(
                    SeverityLevel.WARNING,
                    "Produ√ß√£o zero com tempo significativo registrado",
                    idx, row, 0.8, "generic_production",
                    ["Verificar se houve parada n√£o registrada",
                     "Confirmar se quantidade foi preenchida"]
                )
    
    def _detect_statistical_anomalies(self, df: pd.DataFrame):
        """Detecta anomalias usando an√°lise estat√≠stica simples"""
        # Agrupar por m√°quina
        for machine_name, machine_data in df.groupby('maquina'):
            machine_data = machine_data.copy()
            
            # Detectar outliers em tempos
            tempos = []
            for tempo_str in machine_data['tempo']:
                minutos = self._extract_time_minutes(str(tempo_str))
                if minutos > 0:
                    tempos.append(minutos)
            
            if len(tempos) > 10:  # precisa de dados suficientes
                tempos_array = np.array(tempos)
                q75, q25 = np.percentile(tempos_array, [75, 25])
                iqr = q75 - q25
                lower_bound = q25 - 1.5 * iqr
                upper_bound = q75 + 1.5 * iqr
                
                for idx, row in machine_data.iterrows():
                    tempo_minutos = self._extract_time_minutes(str(row.get('tempo', '00:00')))
                    if tempo_minutos < lower_bound or tempo_minutos > upper_bound:
                        self._add_ai_result(
                            SeverityLevel.INFO,
                            f"Tempo estatisticamente an√¥malo: {tempo_minutos}min",
                            idx, row, 0.8, "statistical_outlier",
                            [f"Faixa normal (IQR): {q25:.0f}-{q75:.0f}min",
                             "Detectado por an√°lise estat√≠stica autom√°tica"]
                        )
    
    def _ai_validate_sequences(self, df: pd.DataFrame):
        """Valida√ß√£o inteligente de sequ√™ncias"""
        for machine in df['maquina'].unique():
            if pd.isna(machine):
                continue
            
            machine_data = df[df['maquina'] == machine].copy()
            machine_data = machine_data.sort_values('inicio')
            
            # Detectar padr√µes suspeitos de sequ√™ncia
            self._detect_sequence_anomalies(machine_data)
    
    def _detect_sequence_anomalies(self, machine_data: pd.DataFrame):
        """Detecta anomalias em sequ√™ncias de opera√ß√µes"""
        setup_count = 0
        production_count = 0
        
        for idx, row in machine_data.iterrows():
            evento = str(row.get('evento', '')).lower()
            processo = str(row.get('processo', '')).lower()
            combined = f"{evento} {processo}"
            
            if self._is_likely_setup(combined):
                setup_count += 1
                if setup_count > 1 and production_count == 0:
                    self._add_ai_result(
                        SeverityLevel.WARNING,
                        "M√∫ltiplos setups sem produ√ß√£o intermedi√°ria",
                        idx, row, 0.9, "sequence_anomaly",
                        ["Padr√£o detectado automaticamente",
                         "Verificar se setups anteriores foram cancelados"]
                    )
                production_count = 0  # reset contador de produ√ß√£o
            
            elif self._is_likely_production(combined):
                production_count += 1
                if production_count == 1 and setup_count == 0:
                    self._add_ai_result(
                        SeverityLevel.INFO,
                        "Produ√ß√£o sem setup recente detectado",
                        idx, row, 0.7, "sequence_pattern",
                        ["Pode indicar continuidade de opera√ß√£o anterior",
                         "Verificar se setup foi registrado corretamente"]
                    )
    
    def _extract_time_minutes(self, time_str: str) -> int:
        """Extra√ß√£o inteligente de tempo em minutos"""
        if pd.isna(time_str) or not time_str:
            return 0
        
        time_str = str(time_str).lower().strip()
        
        # Tentar v√°rios padr√µes
        for pattern in self.base_time_patterns:
            match = re.search(pattern, time_str)
            if match:
                groups = match.groups()
                
                if 'h' in pattern and 'min' in pattern:  # 1h 30min
                    hours = int(groups[0])
                    minutes = int(groups[1]) if groups[1] else 0
                    return hours * 60 + minutes
                elif ':' in pattern:  # 1:30
                    hours = int(groups[0])
                    minutes = int(groups[1])
                    return hours * 60 + minutes
                elif 'min' in pattern:  # 90min
                    return int(groups[0])
                elif 'h' in pattern:  # 2h
                    return int(groups[0]) * 60
                elif 'p/h' in pattern or 'p h' in pattern:  # velocidade, n√£o tempo
                    continue
        
        return 0
    
    def _safe_float(self, value) -> float:
        """Convers√£o segura para float"""
        try:
            if pd.isna(value):
                return 0.0
            return float(str(value).replace(',', '.'))
        except:
            return 0.0
    
    def _normalize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normaliza√ß√£o inteligente de colunas"""
        df_norm = df.copy()
        df_norm.columns = [col.lower().strip() for col in df_norm.columns]
        
        # Mapeamento flex√≠vel
        column_mapping = {
            r'in√≠cio|inicio': 'inicio',
            r't√©rmino|termino|fim': 'termino',
            r'm√°quina|maquina|equipamento': 'maquina',
            r'qtd\.?\s*recebida|quantidade\s*recebida': 'qtd_recebida',
            r'qtd\.?\s*produzida|quantidade\s*produzida': 'qtd_produzida',
            r'prod\.?\s*p?\/?h|produ√ß√£o\s*hora': 'prod_hora',
            r'qtd\.?\s*entregue|quantidade\s*entregue': 'qtd_entregue',
            r'observa√ß√µes|observacoes|obs': 'observacoes'
        }
        
        for pattern, standard_name in column_mapping.items():
            for col in df_norm.columns:
                if re.search(pattern, col, re.IGNORECASE):
                    df_norm = df_norm.rename(columns={col: standard_name})
                    break
        
        return df_norm
    
    def _check_time_consistency(self, idx: int, row: pd.Series):
        """Valida√ß√£o b√°sica de consist√™ncia de tempo"""
        try:
            inicio = pd.to_datetime(row.get('inicio'))
            termino = pd.to_datetime(row.get('termino'))
            tempo_str = str(row.get('tempo', '00:00'))
            
            tempo_real = termino - inicio
            tempo_informado_min = self._extract_time_minutes(tempo_str)
            tempo_informado = timedelta(minutes=tempo_informado_min)
            
            diff_minutes = abs(tempo_real.total_seconds() - tempo_informado.total_seconds()) / 60
            
            if diff_minutes > 10:  # toler√¢ncia de 10 minutos
                confidence = 0.9 if diff_minutes > 60 else 0.7
                self._add_ai_result(
                    SeverityLevel.WARNING,
                    f"Discrep√¢ncia de tempo: calculado {tempo_real} vs informado {tempo_informado}",
                    idx, row, confidence, "time_consistency",
                    ["Verificar se hor√°rios foram apontados corretamente",
                     "Considerar intervalos n√£o computados"]
                )
        except Exception:
            self._add_ai_result(
                SeverityLevel.ERROR,
                "Erro ao processar hor√°rios - formato inv√°lido",
                idx, row, 1.0, "data_format",
                ["Verificar formato de data/hora", "Corrigir dados inv√°lidos"]
            )
    
    def _check_data_completeness(self, idx: int, row: pd.Series):
        """Verifica√ß√£o de completude com prioriza√ß√£o inteligente"""
        # Campos cr√≠ticos vs opcionais
        critical_fields = ['inicio', 'termino', 'evento', 'maquina']
        important_fields = ['operador', 'tempo']
        optional_fields = ['processo', 'qtd_produzida', 'qtd_recebida']
        
        missing_critical = []
        missing_important = []
        
        for field in critical_fields:
            if pd.isna(row.get(field)) or str(row.get(field)).strip() == '':
                missing_critical.append(field)
        
        for field in important_fields:
            if pd.isna(row.get(field)) or str(row.get(field)).strip() == '':
                missing_important.append(field)
        
        if missing_critical:
            self._add_ai_result(
                SeverityLevel.ERROR,
                f"Campos cr√≠ticos ausentes: {', '.join(missing_critical)}",
                idx, row, 1.0, "data_completeness",
                ["Preencher campos obrigat√≥rios para valida√ß√£o",
                 "Sistema n√£o pode funcionar sem estes dados"]
            )
        
        if missing_important:
            self._add_ai_result(
                SeverityLevel.WARNING,
                f"Campos importantes ausentes: {', '.join(missing_important)}",
                idx, row, 0.8, "data_quality",
                ["Dados incompletos reduzem precis√£o da an√°lise",
                 "Recomendado preencher para melhor controle"]
            )
    
    def _add_ai_result(self, severity: SeverityLevel, message: str, idx: int, 
                      row: pd.Series, confidence: float, pattern_type: str, 
                      suggestions: List[str]):
        """Adiciona resultado com informa√ß√µes de IA"""
        self.results.append(ValidationResult(
            severity=severity,
            message=message,
            row_index=idx,
            machine=str(row.get('maquina', 'N/A')),
            operator=str(row.get('operador', 'N/A')),
            timestamp=str(row.get('inicio', 'N/A')),
            suggestions=suggestions,
            confidence=confidence,
            pattern_type=pattern_type
        ))
    
    def export_learned_patterns(self, filepath: str = "machine_patterns.json"):
        """Exporta padr√µes aprendidos para reuso"""
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'machine_profiles': {},
            'global_patterns': self.global_patterns
        }
        
        for name, profile in self.machine_profiles.items():
            export_data['machine_profiles'][name] = {
                'setup_times': dict(profile.setup_times),
                'production_speeds': profile.production_speeds,
                'common_processes': dict(profile.common_processes),
                'typical_operators': list(profile.typical_operators),
                'setup_keywords': profile.setup_keywords,
                'anomaly_thresholds': profile.anomaly_thresholds,
                'last_updated': profile.last_updated.isoformat()
            }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        return f"Padr√µes exportados para {filepath}"
    
    def import_learned_patterns(self, filepath: str):
        """Importa padr√µes previamente aprendidos"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.global_patterns = data.get('global_patterns', {})
            
            for name, profile_data in data.get('machine_profiles', {}).items():
                profile = MachineProfile(name=name)
                profile.setup_times = profile_data.get('setup_times', {})
                profile.production_speeds = profile_data.get('production_speeds', [])
                profile.common_processes = Counter(profile_data.get('common_processes', {}))
                profile.typical_operators = set(profile_data.get('typical_operators', []))
                profile.setup_keywords = profile_data.get('setup_keywords', [])
                profile.anomaly_thresholds = profile_data.get('anomaly_thresholds', {})
                profile.last_updated = datetime.fromisoformat(profile_data.get('last_updated', datetime.now().isoformat()))
                
                self.machine_profiles[name] = profile
            
            return f"Padr√µes importados de {filepath}"
        
        except Exception as e:
            return f"Erro ao importar padr√µes: {str(e)}"
    
    def generate_ai_report(self, results: List[ValidationResult] = None) -> str:
        """Gera relat√≥rio inteligente com insights de IA"""
        if results is None:
            results = self.results
        
        if not results:
            return "‚úÖ Nenhuma anomalia detectada pela IA!"
        
        report = ["ü§ñ RELAT√ìRIO DE VALIDA√á√ÉO IA AVAN√áADA", "=" * 55, ""]
        
        # Estat√≠sticas de confian√ßa
        high_confidence = [r for r in results if r.confidence >= 0.8]
        medium_confidence = [r for r in results if 0.5 <= r.confidence < 0.8]
        low_confidence = [r for r in results if r.confidence < 0.5]
        
        report.append("üìä CONFIAN√áA DA IA:")
        report.append(f"   üéØ Alta confian√ßa (‚â•80%): {len(high_confidence)} detec√ß√µes")
        report.append(f"   üîç M√©dia confian√ßa (50-80%): {len(medium_confidence)} detec√ß√µes")
        report.append(f"   ‚ùì Baixa confian√ßa (<50%): {len(low_confidence)} detec√ß√µes")
        report.append("")
        
        # An√°lise por tipo de padr√£o
        pattern_counts = Counter(r.pattern_type for r in results)
        report.append("üîç TIPOS DE ANOMALIAS DETECTADAS:")
        for pattern, count in pattern_counts.most_common():
            emoji_map = {
                'statistical_anomaly': 'üìà',
                'performance_anomaly': '‚ö°',
                'sequence_anomaly': 'üîó',
                'operator_pattern': 'üë®‚Äçüîß',
                'machine_pattern': 'üè≠',
                'time_consistency': '‚è∞',
                'data_completeness': 'üìù'
            }
            emoji = emoji_map.get(pattern, 'üîç')
            report.append(f"   {emoji} {pattern.replace('_', ' ').title()}: {count}")
        report.append("")
        
        # Resumo por severidade
        severity_counts = Counter(r.severity for r in results)
        report.append("‚ö†Ô∏è SEVERIDADE:")
        for severity, count in severity_counts.items():
            emoji = {"INFO": "‚ÑπÔ∏è", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "CRITICAL": "üö®"}
            report.append(f"   {emoji.get(severity.value)} {severity.value}: {count}")
        report.append("")
        
        # Insights inteligentes
        report.extend(self._generate_ai_insights(results))
        
        # Detalhes por m√°quina (s√≥ alta/m√©dia confian√ßa)
        priority_results = [r for r in results if r.confidence >= 0.5]
        if priority_results:
            report.extend(["", "üîç DETEC√á√ïES PRIORIT√ÅRIAS:", ""])
            
            by_machine = defaultdict(list)
            for result in priority_results:
                by_machine[result.machine].append(result)
            
            for machine, machine_results in by_machine.items():
                report.append(f"üè≠ {machine.upper()}")
                report.append("-" * 40)
                
                for result in sorted(machine_results, key=lambda x: x.confidence, reverse=True):
                    confidence_bar = "‚ñà" * int(result.confidence * 10)
                    severity_emoji = {"INFO": "‚ÑπÔ∏è", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "CRITICAL": "üö®"}
                    
                    report.append(f"{severity_emoji.get(result.severity.value)} {result.message}")
                    report.append(f"   üìä Confian√ßa: {confidence_bar} {result.confidence:.1%}")
                    report.append(f"   üìÖ {result.timestamp} | üë®‚Äçüîß {result.operator}")
                    report.append(f"   üéØ Padr√£o: {result.pattern_type.replace('_', ' ')}")
                    
                    if result.suggestions:
                        report.append("   üí° A√ß√µes recomendadas:")
                        for suggestion in result.suggestions[:2]:  # limitar sugest√µes
                            report.append(f"      ‚Ä¢ {suggestion}")
                    report.append("")
                
                report.append("")
        
        # M√°quinas aprendidas
        if self.machine_profiles:
            report.extend(["", "üß† CONHECIMENTO ADQUIRIDO:", ""])
            for name, profile in self.machine_profiles.items():
                setup_count = sum(len(times) for times in profile.setup_times.values())
                speed_count = len(profile.production_speeds)
                
                report.append(f"üè≠ {name.upper()}:")
                report.append(f"   üìö {setup_count} setups analisados")
                report.append(f"   ‚ö° {speed_count} velocidades registradas")
                report.append(f"   üë• {len(profile.typical_operators)} operadores conhecidos")
                
                if profile.anomaly_thresholds:
                    if 'setup_mean' in profile.anomaly_thresholds:
                        report.append(f"   ‚è±Ô∏è Setup m√©dio: {profile.anomaly_thresholds['setup_mean']:.0f}min")
                    if 'speed_mean' in profile.anomaly_thresholds:
                        report.append(f"   üöÄ Velocidade m√©dia: {profile.anomaly_thresholds['speed_mean']:.0f} p/h")
                
                report.append("")
        
        return "\n".join(report)
    
    def _generate_ai_insights(self, results: List[ValidationResult]) -> List[str]:
        """Gera insights inteligentes baseados nos padr√µes detectados"""
        insights = ["üß† INSIGHTS DA IA:", ""]
        
        # An√°lise temporal
        high_conf_results = [r for r in results if r.confidence >= 0.7]
        if len(high_conf_results) > len(results) * 0.6:
            insights.append("‚úÖ Alta confian√ßa geral nas detec√ß√µes - dados consistentes com padr√µes hist√≥ricos")
        else:
            insights.append("‚ö†Ô∏è Muitas detec√ß√µes com baixa confian√ßa - poss√≠vel mudan√ßa nos padr√µes ou dados at√≠picos")
        
        # An√°lise de m√°quinas
        machine_issues = defaultdict(int)
        for result in results:
            if result.severity in [SeverityLevel.WARNING, SeverityLevel.ERROR]:
                machine_issues[result.machine] += 1
        
        if machine_issues:
            worst_machine = max(machine_issues.items(), key=lambda x: x[1])
            insights.append(f"üéØ M√°quina com mais anomalias: {worst_machine[0]} ({worst_machine[1]} problemas)")
        
        # An√°lise de padr√µes
        pattern_counts = Counter(r.pattern_type for r in results)
        if pattern_counts:
            top_pattern = pattern_counts.most_common(1)[0]
            pattern_desc = {
                'statistical_anomaly': 'anomalias estat√≠sticas - valores fora do padr√£o hist√≥rico',
                'performance_anomaly': 'problemas de performance - velocidades at√≠picas',
                'sequence_anomaly': 'problemas de sequ√™ncia - fluxo irregular de opera√ß√µes',
                'operator_pattern': 'padr√µes de operador - turnos ou pessoas at√≠picas',
                'time_consistency': 'inconsist√™ncias de tempo - discrep√¢ncias nos hor√°rios'
            }
            desc = pattern_desc.get(top_pattern[0], 'padr√£o n√£o categorizado')
            insights.append(f"üîç Padr√£o mais comum: {desc}")
        
        # Recomenda√ß√µes
        insights.extend(["", "üí° RECOMENDA√á√ïES GERAIS:"])
        
        error_count = len([r for r in results if r.severity == SeverityLevel.ERROR])
        if error_count > 0:
            insights.append("üö® Corrigir erros cr√≠ticos primeiro - podem afetar confiabilidade do sistema")
        
        if len(self.machine_profiles) < 3:
            insights.append("üìà Coletar mais dados hist√≥ricos para melhorar precis√£o da IA")
        
        low_conf_count = len([r for r in results if r.confidence < 0.5])
        if low_conf_count > len(results) * 0.3:
            insights.append("üéØ Revisar manualmente detec√ß√µes de baixa confian√ßa")
        
        insights.append("")
        return insights

# Classe auxiliar para an√°lise avan√ßada
class ProductionAnalytics:
    """An√°lises avan√ßadas de produ√ß√£o usando os dados validados"""
    
    def __init__(self, validator: SmartProductionValidator):
        self.validator = validator
    
    def analyze_efficiency_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Analisa tend√™ncias de efici√™ncia por m√°quina"""
        df = self.validator._normalize_columns(df)
        trends = {}
        
        for machine in df['maquina'].unique():
            if pd.isna(machine):
                continue
            
            machine_data = df[df['maquina'] == machine].copy()
            machine_data['date'] = pd.to_datetime(machine_data['inicio']).dt.date
            
            daily_efficiency = []
            for date, day_data in machine_data.groupby('date'):
                total_time = 0
                total_production = 0
                
                for _, row in day_data.iterrows():
                    tempo_min = self.validator._extract_time_minutes(str(row.get('tempo', '0')))
                    qtd_prod = self.validator._safe_float(row.get('qtd_produzida', 0))
                    
                    if 'produ√ß√£o' in str(row.get('evento', '')).lower():
                        total_time += tempo_min
                        total_production += qtd_prod
                
                if total_time > 0:
                    efficiency = (total_production / total_time) * 60  # pe√ßas/hora
                    daily_efficiency.append({'date': date, 'efficiency': efficiency})
            
            if daily_efficiency:
                trends[machine] = {
                    'daily_data': daily_efficiency,
                    'avg_efficiency': np.mean([d['efficiency'] for d in daily_efficiency]),
                    'trend_direction': self._calculate_trend([d['efficiency'] for d in daily_efficiency])
                }
        
        return trends
    
    def _calculate_trend(self, values: List[float]) -> str:
        """Calcula dire√ß√£o da tend√™ncia"""
        if len(values) < 3:
            return "insufficient_data"
        
        x = np.arange(len(values))
        slope = np.polyfit(x, values, 1)[0]
        
        if slope > 0.1:
            return "improving"
        elif slope < -0.1:
            return "declining"
        else:
            return "stable"
    
    def detect_bottlenecks(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Detecta gargalos de produ√ß√£o"""
        df = self.validator._normalize_columns(df)
        bottlenecks = []
        
        # Analisar tempos de setup por m√°quina
        for machine in df['maquina'].unique():
            if pd.isna(machine):
                continue
            
            machine_data = df[df['maquina'] == machine]
            setup_times = []
            
            for _, row in machine_data.iterrows():
                evento = str(row.get('evento', '')).lower()
                if 'acerto' in evento or 'setup' in evento:
                    tempo_min = self.validator._extract_time_minutes(str(row.get('tempo', '0')))
                    if tempo_min > 0:
                        setup_times.append(tempo_min)
            
            if setup_times:
                avg_setup = np.mean(setup_times)
                if avg_setup > 120:  # mais de 2 horas
                    bottlenecks.append({
                        'machine': machine,
                        'type': 'long_setup',
                        'avg_time': avg_setup,
                        'impact': 'high' if avg_setup > 180 else 'medium'
                    })
        
        return bottlenecks

# Exemplo de uso avan√ßado
def advanced_example():
    """Exemplo de uso do sistema IA avan√ßado"""
    
    # Dados de exemplo mais complexos
    sample_data = {
        'inicio': [
            '06/06/2025 08:04', '06/06/2025 09:06', '06/06/2025 09:14',
            '06/06/2025 10:30', '06/06/2025 11:45', '06/06/2025 13:00',
            '07/06/2025 08:00', '07/06/2025 09:30', '07/06/2025 10:15'
        ],
        'termino': [
            '06/06/2025 09:06', '06/06/2025 09:14', '06/06/2025 10:20',
            '06/06/2025 11:30', '06/06/2025 12:45', '06/06/2025 14:30',
            '07/06/2025 09:15', '07/06/2025 10:10', '07/06/2025 12:00'
        ],
        'tempo': [
            '01:01', '00:07', '01:06', '01:00', '01:00', '01:30',
            '01:15', '00:40', '01:45'
        ],
        'evento': [
            '01 Acerto', '02 Produ√ß√£o', '0 Ocioso',
            '01 Acerto', '02 Produ√ß√£o', '02 Produ√ß√£o',
            '01 Acerto', '02 Produ√ß√£o', '02 Produ√ß√£o'
        ],
        'processo': [
            '1 h 30 min - Speedmaster CD102', 'CMYK', 'AGUARDANDO PAPEL',
            'faca nova - Bobst Diana', 'Corte e vinco', 'Corte e vinco',
            'acerto verniz - Sakurai SC102', 'Verniz UV', 'Verniz UV'
        ],
        'qtd_recebida': [0, 789, 0, 0, 1200, 800, 0, 500, 300],
        'qtd_produzida': [0, 729, 0, 0, 1150, 780, 0, 480, 290],
        'maquina': [
            'Speedmaster CD 102', 'Speedmaster CD 102', 'Speedmaster CD 102',
            'Bobst Diana 1050', 'Bobst Diana 1050', 'Bobst Diana 1050',
            'Sakurai SC 102', 'Sakurai SC 102', 'Sakurai SC 102'
        ],
        'operador': [
            'Augusto', 'Augusto', 'Augusto',
            'Carlos', 'Carlos', 'Maria',
            'Jo√£o', 'Jo√£o', 'Pedro'
        ]
    }
    
    df = pd.DataFrame(sample_data)
    
    # Criar validador IA
    ai_validator = SmartProductionValidator()
    
    # Fase 1: Aprender padr√µes
    print("üß† APRENDENDO PADR√ïES...")
    learning_report = ai_validator.analyze_and_learn(df)
    print(f"M√°quinas descobertas: {learning_report['machines_discovered']}")
    print(f"Confian√ßa: {learning_report['confidence_level']:.1%}")
    
    # Fase 2: Validar com IA
    print("\nüîç VALIDANDO COM IA...")
    results = ai_validator.validate_with_ai(df)
    
    # Relat√≥rio IA
    print(ai_validator.generate_ai_report(results))
    
    # An√°lise avan√ßada
    analytics = ProductionAnalytics(ai_validator)
    trends = analytics.analyze_efficiency_trends(df)
    bottlenecks = analytics.detect_bottlenecks(df)
    
    if trends:
        print("üìà AN√ÅLISE DE EFICI√äNCIA:")
        for machine, data in trends.items():
            print(f"   {machine}: {data['avg_efficiency']:.0f} p/h ({data['trend_direction']})")
    
    if bottlenecks:
        print("\nüö´ GARGALOS DETECTADOS:")
        for bottleneck in bottlenecks:
            print(f"   {bottleneck['machine']}: {bottleneck['type']} ({bottleneck['avg_time']:.0f}min)")
    
    # Exportar conhecimento
    ai_validator.export_learned_patterns("ai_patterns.json")
    print("\nüíæ Padr√µes salvos em 'ai_patterns.json'")

if __name__ == "__main__":
    advanced_example()